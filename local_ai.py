from llama_cpp import Llama

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Phi-2 (–ª–æ–∫–∞–ª—å–Ω–æ)
print("üß† –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –º–æ–¥–µ–ª—å Phi-2... –¶–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ –∫—ñ–ª—å–∫–∞ —Å–µ–∫—É–Ω–¥.")

llm = Llama(
    model_path="phi-2.Q3_K_S.gguf",  # –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ
    n_ctx=1024,
    n_threads=4,
    n_gpu_layers=0  # 0 ‚Äî –±–æ –±–µ–∑ GPU
)

def ask(prompt):
    """–û—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –ª–æ–∫–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
    output = llm(
        f"Instruct: {prompt}\nOutput:",
        max_tokens=200,
        temperature=0.7,
        top_p=0.9,
        echo=False,
    )
    text = output["choices"][0]["text"]
    return text.strip()
